---
title: "Procesamiento 3"
author: "Maria Candelaria Dorta Delgado - Andres Felipe Beltran"
date: "5/11/2021"
output:
   rmdformats::downcute
---

En este documento se presenta el flujo de trabajo utilizado para procesar los espectros compartidos en las carpetas `3_T 50` y `3 SST50`

Primero, creamos una lista con los nombres de los archivos contenidos en esta carpeta, para poder importarlos a `R`:

```{r}
nombres <- list.files(, pattern = 'CSV')
nombres
```

Revisamos si los triplicados estan completos:

```{r}
(length(nombres)-1)/3
```
Hasta el momento, hay un espectro repetido:

```{r}
nombres[46]
```

Vamos a continuar sin este espectro por el momento:

```{r}
nombres <- nombres[-46]
```

Procedemos a utilizr la función `read.csv` para leer los archivos `.CSV` y crear objetos dentro de R que van a contener su información.

Ya que `nombres` es una lista, podemos utilizar la función de ciclo `lapply` para aplicar de manera iterativa la función `read.csv` a todos los elementos de la lista `nombres`, guardando el resultado de cada iteración en cada elemento de la lista `data`.


```{r}
data <- lapply(nombres, read.csv, header = FALSE)
```

`data` es un objeto de clase `list` que contiene en cada posición, un `data.frame` con la información de los `.csv`. Es decir, una columna con los números de onda, y otra con las absorbancias:

```{r}
str(data[[1]])
```
En este punto, podemos guardar la información de los números de onda en un objeto llamado `wavenumber`

```{r}
wavenumber <- data[[1]][,1]
```

En el chunk anterior guardamos lo que hay en la primera columna `[,1]` del primer elemento de `data`: `data[[1]]` 

Ahora, podemos volver a aplicar la función `lapply`, en este caso a la lista `data` para extraer solo la segunda columna, que es la que nos interesa para construir nuestro data.frame para análisis quimiométrico.

```{r}
data2 <- lapply(data, '[', 2)
```

Al usar la función `lapply`, de nuevo creamos una lista. podemos convertir la lista `data2` en un `data.frame` mediante la función `as.data.frame`.

```{r}
data2 <- as.data.frame(data2)
dim(data2)
names(data2) <- nombres
```
Ahora tenemos un `data.frame` con 1869 filas y 120 columnas, los triplicados de 40 muestras.

Ahora podemos proceder a crear la transpuesta de este data.frame utilizando la función `t()`:

```{r}
datat <- t(data2)
```

`datat` es un objeto de clase `matrix`,  en cuyos `rownames` contiene los nombres de las muestras, como vienen en los nombres de los archivos `.CSV`.

Ahora, recuperamos la información almacenada en el objeto `wavenumber` para nombrar las columnas, o variables del objeto `datat`

```{r}
colnames(datat) <- wavenumber
```

Ahora, para encontrar los espectros a los cuales les vamos a calcular el promedio, utilizamos una matriz de búsqueda, conteniendo los niveles que diferencian las muestras, en este caso son:

* Zona de muestreo: `UF1` `UF2` `UF3` `UF4` `UF5` y `LF1` `LF2` `LF3` `LF4` `LF5`.
* tiempo de molienda: `A` y `B`.
* con o sin solución salina `SS` y ` `.

```{r}
set.seed(0)

searchGrid <- expand.grid(Sampling = c('UF1',
                                       'UF2',
                                       'UF3',
                                       'UF4',
                                       'UF5',
                                       'LF1',
                                       'LF2',
                                       'LF3',
                                       'LF4',
                                       'LF5'),
                          millingTime = c('A',
                                          'B'),
                          brine = c('SS',
                                    ''))

# knitr::kable(searchGrid)
```

En la matriz `searchGrid` tenemos un total de 40 combinaciones para los nombres de las muestras, según las distintas variaciones de los tratamientos. 

Vamos a utilizar esta matriz, para buscar entre todos los nombres de todos los espectros, y encontrar los tres triplicados para cada variación del experimento:

Primero, vamos a crear un vector que va a contener los nuevos nombres que identifican cada variación del experimento:

```{r}
rownames.prom <- character(40)
```

Luego, mediante un ciclo `for` podemos guardar en cada posición de `rownames.prom` la unión de las tres variables para cada experimento:

```{r}
for(i in 1:40){
  
  rownames.prom[i] <- paste(searchGrid[i,1],
      searchGrid[i,2],
      searchGrid[i,3])
}
rownames.prom
```

Ahora, creamos la matriz en la cual vamos a guardar los resultados el promedio de cada triplicado:

```{r}
prom <- matrix(,
               ncol = ncol(datat),
               nrow = nrow(datat)/3)
```

Ahora, vamos a crear una lista llamada `index`. En esta lista, vamos a guardar las 3 posiciones dentro de la lista `nombres` que van a coincidir con las expresiones regulares dictadas por `searchGrid` para cada variación del experimento. 

Por ejemplo, para `UF1 A SS`,  la posición correspondiente en la lista `index` va a contener 3 elementos que nos dicen las posiciones para los espectros  `UF1 A SS 1`, `UF1 A SS 2`, y `UF1 A SS 3` dentro de los nombres de las filas de `datat`.

```{r}
index <- vector('list',40)
```

Ahora, vamos a utilizar una función de búsqueda, anidada con una función de comparación.

* `grepl` función de búsqueda que utiliza expresiones regulares (variables según la búsqueda) junto con caracteres (provenientes de `searchGrid`) como input
* `which` función de comparación que nos dice si se cumple o no una condición, pasa como argumento el output de `grepl` que nos dice si sí o no coincide el elemento en cuestión con el criterio de búsqueda.

```{r }
for(i in 1:40){
  
  index[[i]] <- c(
                  which(
                         grepl(
                                paste0('(?=.*',
                                      as.character(searchGrid[i,1]),
                                      ')(?=.*',
                                      as.character(searchGrid[i,2]),
                                      ')(?=.*',
                                       as.character(searchGrid[i,3]),
                                      ')'
                                  
                                       ),
                rownames(datat), perl = T              )
    
                       )
                  )
}
```

podemos confirmar los resultados antes de calcular. Por ejemplo, para la primera fila de `searchGrid` :

```{r}
searchGrid[1,]
```
Ya tenemos ubicados los espectros que corresponden a esta combinación:

```{r}
index[[1]]
```
confirmando en los nombres de las muestras de `datat`:
```{r}
rownames(datat)[index[[1]]]
```

Corresponde a los tres triplicados de la muestra `UF1`, la cual tuvo un tiempo de molienda de 2 a 4 minutos, y una extracción con solución salina.


Podemos proceder a calcular los promedios:

```{r}
for(j in 1:length(colnames(datat))){
  
for(i in 1:40){

prom[i,j] <- mean(c(datat[index[[i]][1],j],
                    datat[index[[i]][2],j],
                    datat[index[[i]][3],j]
                      ) )
}
}

rownames(prom) <- rownames.prom
colnames(prom) <- wavenumber
```


Una vez calculados, podemos graficar los promedios:

Podemos diferenciar los espectros de las muestras a las cuales se les realizo la extracción con solución salina mediante una revisión

```{r}
NaCl <- vector('character', length(rownames(prom)))

for(i in 1:40){
  
 if(grepl('SS', rownames(prom)[i],perl =T)){NaCl[i]<- 'red'}
  else{NaCl[i]<- 'black'}
  
}


indexSS <- vector('logical',40)

 indexSS<-which(grepl('SS', rownames(prom),perl =T))
  


```


```{r}
for(i in 1:40){
  plot(wavenumber,
      prom[i,],
       xlab = 'wave number cm-1',
       ylab = 'absorbance a.u.',
       xlim = c(3900,400),
       ylim = c(0,0.12),
       type = 'l',
       col = NaCl[i])
  par(new = T)

  
}
```

Podemos seleccionar este rango , de 1700 a 400 $cm^{-1}$ creando un nuevo objeto para los promedios y para los números de ondaÑ

```{r}
colnames(prom)[c(1,676)]
```
```{r}
Rango1 <- prom[,1:676]
wn.Rango1 <- wavenumber[1:676]

```

```{r}
Rango2 <- Rango1[, -c(1:163)]
wn.Rango2 <- wn.Rango1[-c(1:163)] # Quitando desde 790 hacia abajo
```


Para eliminar la variabilidad por ruido instrumental, podemos calcular y restar la línea base para cada uno de los promedios, utilizando el método `rubberband` disponible en el paquete de R `hyperSpec`, desarrollado para quimiometría aplicada a imágenes hiper-espectrales.



```{r message=FALSE}
library(hyperSpec)
spc <- new('hyperSpec', spc = Rango2, wavelength = wn.Rango2)
bend <- 0.1 * wl.eval (spc, function (x) x^6+x^5+x^4+x^3+x^2, normalize.wl=normalize01)

bl <- spc.rubberband (spc+bend, noise = 1e-4, df=20)-bend
suma <- spc+bend
spc3 <- spc - bl

plot(spc, wl.reverse = TRUE)
plot(bl, add=TRUE, col=2,wl.reverse = TRUE)

plot(suma,wl.reverse = TRUE)
plot(bend, add=TRUE, col=2,wl.reverse = TRUE)
plot(spc3,wl.reverse = TRUE)

corregido <- as.data.frame(spc3[1:40])
corregido2 <- as.data.frame(corregido[,1])
```
Hasta este punto se conserva una parte de la variabilidad por ruido instrumental en el rango de 790 a 400 cm-1. Dado a que en el rango de 1700 a 790 cm-1 tenemos información de enlaces siloxano en dos bandas, podemos utilizar este rango para el analisis multivariado:




<!-- En el procedimiento de corrección de línea base, observamos que uno de los espectros promedio tiene una deriva instrumental mucho más notoria que los demás, podemos preguntarle a R cual de los espectros presenta estos valores de absorbancia tan altos: -->

<!-- ```{r} -->
<!-- which(prom[,1]>0.2) -->
<!-- ``` -->
<!-- Sabiendo que es el elemento en la lista de promedios número 23, podemos preguntar a `index` a partir de que espectros se calculó el promedio, y graficar los tres para ver si los tres presentan el mismo comportamiento. -->

<!-- ```{r} -->
<!-- index[[23]] -->
<!-- rownames(prom)[23] -->
<!-- ``` -->
<!-- ```{r} -->
<!-- rownames(datat)[c(22,23,24)] -->
<!-- ``` -->
<!-- ```{r} -->
<!-- for(i in 1:3){ -->
<!--   plot(wavenumber, -->
<!--        datat[index[[23]][i],], -->
<!--         xlab = 'wave number cm-1', -->
<!--        ylab = 'absorbance a.u.', -->
<!--        xlim = c(1700,400), -->
<!--        ylim = c(0,1), -->
<!--        type = 'l') -->
<!--   par(new = T) -->




<!-- } -->
<!-- ``` -->

<!-- Con esto podemos confirmar que la desviación por deriva instrumental no se debe a una de las replicas, si no a la muestra `UF3 A`. A pesar de que con el tratamiento de corrección de línea base se elimina este problema, es importante confirmar cual es la muestra que presenta esta deriva. -->

Podemos graficar los espectros corregidos para este experimento:

```{r}

for(i in 1:length(rownames(corregido2))){
  plot(wn.Rango2,
       corregido2[i,],
       xlab = 'wave number cm-1',
       ylab = 'absorbance a.u.',
       xlim = c(1700,790),
       ylim = c(0,0.06),
       type = 'l',
       col = NaCl[i])
  par(new = T)
  

}
legend('topleft',
       c('50C Colombia SS','50C Colombia'),
       col = c('red','black'),
       lty = 1)
```

```{r}
pca.bl <- prcomp(corregido2, scale =T, center = T)
vp.bl <- (pca.bl$sdev)^2
varianza.bl <- round(vp.bl/sum(vp.bl)*100, 2)
varianza.bl
coord.bl <- pca.bl$x
loadings <- pca.bl$rotation
```
```{r}
library(RColorBrewer)
for(i in 1:2){
  plot(as.numeric(colnames(corregido2)),
       loadings[,i],
       type = 'l',
       ylim = c(-0.15,0.2),
       xlim = c(1700,790),
       ylab = 'PC Loadings',
       xlab = expression(paste("Wave number (cm"^"-1",")")),
       col= brewer.pal(3,'Dark2')[i],
       
       
  )
  
  par(new =T)  
}
abline(h=0)
legend('topleft',
       c('PC1','PC2'),
       col = brewer.pal(3,'Dark2')[1:2],
       lty = 1,
       lw = 2)
```



# Derivatización de espectros

```{r}

library(prospectr)
w2 <- 40
sg <- matrix(ncol = ncol(corregido2)-w2, nrow= nrow(corregido2))
sg <- as.data.frame(sg) 
# for (i in 1:31){
sg<- savitzkyGolay(X = corregido2
                        ,m = 2,
                        p = 3,
                        w = w2+1,
                        delta.wav=2)
# }
# colnames(sg) <- colnames( savitzkyGolay(X = corregido2[1,]
#                         ,m = 2,
#                         p = 3,
#                         w = w2+1,
#                         delta.wav=2))
rownames(sg) <- rownames(corregido2)
# win.graph()
for(i in 1:length(rownames(sg))){
  
  plot(as.numeric(colnames(sg)),
       sg[i,],
       xlab = 'wave number cm-1',
       ylab = '2nd derivative of abs',
       xlim = c(1700,790),
        ylim = c(-5e-05,5e-05),
       type = 'l',
       col = NaCl[i])
  par(new = T)
  
  
}
legend('topleft',
       c('50C Colombia SS','50C Colombia'),
       col = c('red','black'),
       lty = 1)
```



# Análisis exploratorio

## Cambio de nombres para hca

```{r}
nombres.hca <- vector('character', 40)

for (i in 1:40){
  
  if(grepl('(?=.*UF)(?=.*A)((?!SS).)*$',rownames(sg)[i],perl=T)){nombres.hca[i]<- '@50C-Colombia 2-4 min'}else{
     
  if(grepl('(?=.*UF)(?=.*B)((?!SS).)*$',rownames(sg)[i],perl=T)){nombres.hca[i]<- '@50C-Colombia 6-8 min'}else{
    
     if(grepl('(?=.*UF)(?=.*A)(?=.*SS)',rownames(sg)[i],perl=T)){nombres.hca[i]<- '@50C-Colombia 2-4 min SS'}else{
         if(grepl('(?=.*UF)(?=.*B)(?=.*SS)',rownames(sg)[i],perl=T)){nombres.hca[i]<- '@50C-Colombia 6-8 min SS'}else{
    
            if(grepl('(?=.*LF)(?=.*A)((?!SS).)*$',rownames(sg)[i],perl=T)){nombres.hca[i]<- '50C-Colombia 2-4 min'}else{
    
                if(grepl('(?=.*LF)(?=.*B)((?!SS).)*$',rownames(sg)[i],perl=T)){nombres.hca[i]<- '50C-Colombia 6-8 min'}else{
                  
   if(grepl('(?=.*LF)(?=.*A)(?=.*SS)',rownames(sg)[i],perl=T)){nombres.hca[i]<- '50C-Colombia 2-4 min SS'}else{
    
     
     if(grepl('(?=.*LF)(?=.*B)(?=.*SS)',rownames(sg)[i],perl=T)){nombres.hca[i]<- '50C-Colombia 6-8 min SS'}else{
    
     
     
    
  }    
    
  }   
    
  }  
  }  
  } 
    
  } 
  }
    
  }
  
  
}
```

```{r}
compare <- data.frame(antes = rownames(sg), ahora = nombres.hca)
```


```{r}
rownames(sg) <- nombres.hca
```


```{r}
fraction <- vector('integer',40)
for(i in 1:40){
  
  if(grepl('(?=.*@)',rownames(sg)[i],perl=T)){fraction[i] <- 1}else{fraction[i] <- 2}
  
}
# Upper 1
# Lower 2

```

## HCA

```{r}
df <- sg
library(factoextra)
res.hk <-hkmeans(df, 
                 5,
                 hc.metric =  'minkowski')

fviz_dend(res.hk, cex = 0.6, palette = "jco", 
          rect = TRUE, rect_border = "jco", rect_fill = TRUE, ylim = c(-5e-05,2e-04))
```




```{r}
pca.bl.sg <- prcomp(sg, scale =T, center = T)
vp.bl.sg <- (pca.bl.sg$sdev)^2
varianza.bl.sg <- round(vp.bl.sg/sum(vp.bl.sg)*100, 2)
varianza.bl.sg
coord.bl.sg <- pca.bl.sg$x
```

```{r}


plot(coord.bl.sg[,1],
     coord.bl.sg[,2],
     pch = 19,
     xlab = paste('PC 1 -',as.character(varianza.bl.sg[1]), '%'),
     ylab = paste('PC 2 -',as.character(varianza.bl.sg[2]), '%'),
     col = res.hk$cluster,
     xlim = c(-20,90)
     )
abline(h = 0, v= 0, lty = 2)

x<- coord.bl.sg[,1]
y<- coord.bl.sg[,2]
z<- coord.bl.sg[,3]

x1<- x[res.hk$cluster == 1]
y1<- y[res.hk$cluster == 1]
z1<- z[res.hk$cluster == 1]

library(car)
ellipse(c(mean(x1),mean(y1)),
        cov(cbind(x1,y1)),
        radius= sqrt(qchisq(.95, df =2)), 
        col=1,
        center.pch=FALSE,
        add=TRUE,
        # levels=0.95,
        fill=TRUE,
        lwd=0.8,
        
) 

x2<- x[res.hk$cluster == 2]
y2<- y[res.hk$cluster == 2]
z2<- z[res.hk$cluster == 2]

ellipse(c(mean(x2),mean(y2)),
        cov(cbind(x2,y2)),
        radius= sqrt(qchisq(.95, df =2)), 
        col=2,
        center.pch=FALSE,
        add=TRUE,
        # levels=0.95,
        fill=TRUE,
        lwd=0.8,
        
) 

x3<- x[res.hk$cluster == 3]
y3<- y[res.hk$cluster == 3]
z3<- z[res.hk$cluster == 3]

ellipse(c(mean(x3),mean(y3)),
        cov(cbind(x3,y3)),
        radius= sqrt(qchisq(.95, df =2)), 
        col=3,
        center.pch=FALSE,
        add=TRUE,
        # levels=0.95,
        fill=TRUE,
        lwd=0.8,
        
) 

# text(coord.bl.sg[,1],
#      coord.bl.sg[,2],
#      rownames(coord.bl.sg),
#      pos = 4,
#      cex = 0.5)

```


```{r}
x  <- coord.bl.sg[,1]
y  <- coord.bl.sg[,2]
z  <- coord.bl.sg[,3]
x1 <- x[NaCl=='red']
y1 <- y[NaCl=='red']
z1 <- z[NaCl=='red']
x2 <- x[NaCl=='black']
y2 <- y[NaCl=='black']
z2 <- z[NaCl=='black']

plot(coord.bl.sg[,1],
     coord.bl.sg[,2],
     pch = 19,
     xlab = paste('PC 1 -',as.character(varianza.bl.sg[1]), '%'),
     ylab = paste('PC 2 -',as.character(varianza.bl.sg[2]), '%'),
     col = NaCl
     )
abline(h = 0, v= 0, lty = 2)
library(car)
ellipse(c(mean(x1),mean(y1)),
        cov(cbind(x1,y1)),
        radius= sqrt(qchisq(.95, df =2)), 
        col="red",
        center.pch=FALSE,
        add=TRUE,
        # levels=0.95,
        fill=TRUE,
        lwd=0.8,
        
) 
# ellipse(c(mean(x2),mean(y2)),
#         cov(cbind(x2,y2)),
#         radius= sqrt(qnorm(.95)), 
#         col="black",
#         center.pch=FALSE,
#         add=TRUE,
#         # levels=0.95,
#         fill=TRUE,
#         lwd=0.8,
#         
# ) 
 
```



```{r}
library(mdatools)
mdaPCA <-pca(corregido2, 5, center = TRUE, scale = TRUE)
summary(mdaPCA)
plot(mdaPCA, show.labels = T)
# win.graph()
plotScores(mdaPCA$res$cal, show.labels = TRUE)
```
Ya que mediante PCA para estas muestras en específico, no se obserban agrupamientos notorios, podemos exportar estos resultados y compararlos con los resultados de los experimentos anteriores:

```{r}
write.table(prom, 'SSexp.txt' , sep = '\t')
```

